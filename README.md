
# EVA ‚Äì Frontend üéßüí¨

Interfaz web de EVA (Human Grounded Intelligence) para:

- Grabar audio desde el micr√≥fono.
- Segmentar en *shards* (momentos cortos).
- Enviar cada shard al backend `eva-analysis-service`.
- Visualizar:
  - Transcripci√≥n.
  - Emoci√≥n primaria y etiquetas.
  - Rasgos de la se√±al (RMS, pico, frecuencia, ZCR).
  - An√°lisis sem√°ntico (resumen, topics, tipo de momento, flags).
- Navegar una librer√≠a de clips y ver el detalle de cada uno.

---

## Requisitos

- Node.js 20+ (o LTS reciente).
- npm o pnpm (el proyecto est√° preparado para npm por defecto).
- Backend `eva-analysis-service` corriendo en `http://localhost:5005` (o la URL que configures).

---

## Instalaci√≥n

Clona el repo:

```bash
git clone https://github.com/hildealeman/eva.git
cd eva

Instala dependencias:

npm install


‚∏ª

Configuraci√≥n (.env.local)

Hay un archivo de ejemplo:

cp .env.local.example .env.local

Contenido t√≠pico de .env.local:

NEXT_PUBLIC_EVA_DATA_MODE=local
NEXT_PUBLIC_EVA_ANALYSIS_MODE=local
NEXT_PUBLIC_EVA_LOCAL_ANALYSIS_BASE=http://localhost:5005
NEXT_PUBLIC_SHOW_WAVEFORM_MVP=0

	‚Ä¢	NEXT_PUBLIC_EVA_DATA_MODE:
	‚Ä¢	local ‚Üí episodios le√≠dos desde IndexedDB (EpisodeStore).
	‚Ä¢	api ‚Üí lectura/escritura desde el backend:
	‚Ä¢	GET /episodes
	‚Ä¢	GET /episodes/{id}
	‚Ä¢	PATCH /episodes/{id}
	‚Ä¢	PATCH /shards/{id}
	‚Ä¢	NEXT_PUBLIC_EVA_ANALYSIS_MODE:
	‚Ä¢	local ‚Üí usa `NEXT_PUBLIC_EVA_LOCAL_ANALYSIS_BASE`.
	‚Ä¢	cloud ‚Üí usa `NEXT_PUBLIC_EVA_CLOUD_ANALYSIS_BASE`.
	‚Ä¢	none ‚Üí desactiva an√°lisis.
	‚Ä¢	NEXT_PUBLIC_EVA_LOCAL_ANALYSIS_BASE ‚Üí base URL del backend FastAPI (ej. http://localhost:5005).
	‚Ä¢	NEXT_PUBLIC_SHOW_WAVEFORM_MVP:
	‚Ä¢	0 ‚Üí oculta el placeholder de waveform.
	‚Ä¢	1 ‚Üí muestra el bloque MVP para el waveform.

Las variables NEXT_PUBLIC_... se exponen al navegador, as√≠ que solo se usan para configuraci√≥n de UI / endpoint p√∫blico del backend local.

‚∏ª

Correr en desarrollo

npm run dev

Abrir en el navegador:

http://localhost:3000


‚∏ª

P√°ginas principales
	‚Ä¢	/
	‚Ä¢	Pantalla principal de grabaci√≥n.
	‚Ä¢	Bot√≥n para iniciar/detener grabaci√≥n.
	‚Ä¢	Segmentaci√≥n de audio en shards.
	‚Ä¢	Crea y mantiene un episodio actual (episodeId) mientras escuchas.
	‚Ä¢	Env√≠a shards a POST /analyze-shard en el backend (meta incluye episodeId).
	‚Ä¢	/clips
	‚Ä¢	Lista de episodios (hist√≥rico) usando almacenamiento local (IndexedDB) v√≠a EpisodeStore.
	‚Ä¢	/clips/[id]
	‚Ä¢	Detalle de un episodio:
	‚Ä¢	Header con stats (duraci√≥n total, shards, crisis/followups).
	‚Ä¢	Lista de shards seleccionable.
	‚Ä¢	Transcripci√≥n.
	‚Ä¢	Lectura emocional.
	‚Ä¢	An√°lisis sem√°ntico (‚ÄúAn√°lisis sem√°ntico‚Äù).
	‚Ä¢	Rasgos de la se√±al.
	‚Ä¢	Etiquetas sugeridas din√°micas (topics, emoci√≥n primaria, activaci√≥n, prosodia).

‚∏ª

Estructura destacada
	‚Ä¢	src/app/page.tsx
	‚Ä¢	Home: l√≥gica de grabaci√≥n, env√≠o a backend, panel principal.
	‚Ä¢	src/app/clips/page.tsx
	‚Ä¢	Listado de episodios.
	‚Ä¢	src/app/clips/[id]/page.tsx
	‚Ä¢	Vista detallada de un episodio con lista de shards.
	‚Ä¢	src/components/audio/
	‚Ä¢	LiveLevelMeter.tsx: visualizaci√≥n b√°sica de niveles de entrada.
	‚Ä¢	src/components/emotion/
	‚Ä¢	ShardDetailPanel.tsx: panel principal de detalle emocional/sem√°ntico.
	‚Ä¢	ShardListItem.tsx: item de lista para cada shard.
	‚Ä¢	TagEditor.tsx, EmotionStatusPill.tsx, etc.
	‚Ä¢	src/lib/api/evaAnalysisClient.ts
	‚Ä¢	Cliente para llamar a eva-analysis-service.
	‚Ä¢	Maneja timeouts con AbortController (por defecto 60s).
	‚Ä¢	src/lib/audio/
	‚Ä¢	AudioInputManager, AudioBufferRing, createWavBlob, etc.
	‚Ä¢	src/lib/store/EmoShardStore.ts
	‚Ä¢	Capa de persistencia (IndexedDB) para shards.
	‚Ä¢	src/lib/store/EpisodeStore.ts
	‚Ä¢	Capa de persistencia (IndexedDB) para episodios (EpisodeSummary) y reconstrucci√≥n de EpisodeDetail.
	‚Ä¢	src/types/emotion.ts
	‚Ä¢	Tipos compartidos para emociones, features, semantic, etc (incluye EpisodeSummary/EpisodeDetail).

‚∏ª

Flujo de extremo a extremo
	1.	El usuario abre http://localhost:3000/.
	2.	Inicia una grabaci√≥n desde el micr√≥fono.
	3.	Se crea un episodio actual (episodeId) al iniciar.
	4.	El audio se segmenta en shards (trozos de ~10‚Äì15 segundos).
	5.	Por cada shard:
	‚Ä¢	Se calculan features locales (RMS, ZCR, etc.).
	‚Ä¢	Se construye un FormData y se llama a POST /analyze-shard en el backend.
	‚Ä¢	meta incluye shardId, episodeId, source, startTime, endTime.
	6.	El backend devuelve un ShardAnalysisResult con:
	‚Ä¢	transcript, emotion, signalFeatures, semantic, etc.
	7.	El frontend:
	‚Ä¢	Actualiza el shard en memoria y en IndexedDB.
	‚Ä¢	Refresca campos agregados del episodio (tags, momentTypes, emotion dominante, etc.).
	‚Ä¢	Muestra los resultados en el panel de detalle (ShardDetailPanel).
	8.	En /clips y /clips/[id] se puede revisar el hist√≥rico.

‚∏ª

Desarrollo

Lint:

npm run lint

Build:

npm run build


‚∏ª

Notas
	‚Ä¢	La app est√° pensada como un MVP de laboratorio para explorar EVA (Human Grounded Intelligence).
	‚Ä¢	Se puede extender con:
	‚Ä¢	Waveform real.
	‚Ä¢	Controles de reproducci√≥n.
	‚Ä¢	Filtros por emoci√≥n, momentType, topics.
	‚Ä¢	Exportar sesiones / episodios.
